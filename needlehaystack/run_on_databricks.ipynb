{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "\n",
    "from llm_needle_haystack_tester import LLMNeedleHaystackTester\n",
    "from llm_multi_needle_haystack_tester import LLMMultiNeedleHaystackTester\n",
    "from evaluators import Evaluator, LangSmithEvaluator, OpenAIEvaluator\n",
    "from providers import Anthropic, ModelProvider, OpenAI, Databricks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "\n",
    "os.environ['LANGCHAIN_API_KEY'] = dbutils.secrets.get(scope='<your_scope>', secret='<your_langchain_api_key>')\n",
    "os.environ['NIAH_MODEL_API_KEY'] = dbutils.secrets.get(scope='<your_scope>', secret='<your_databricks_pat_token>')\n",
    "os.environ['NIAH_EVALUATOR_API_KEY'] = dbutils.secrets.get(scope='<your_scope>', secret='<your_open_ai_api_key>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class CommandArgs():\n",
    "    provider: str = \"databricks\"\n",
    "    evaluator_label: str = \"openai\"\n",
    "    model_name: str = \"<model_name>\"\n",
    "    evaluator_model_name: Optional[str] = \"gpt-3.5-turbo-0125\"\n",
    "    needle: Optional[str] = \"\\nThe best thing to do in San Francisco is eat a sandwich and sit in Dolores Park on a sunny day.\\n\"\n",
    "    haystack_dir: Optional[str] = \"PaulGrahamEssays\"\n",
    "    retrieval_question: Optional[str] = \"What is the best thing to do in San Francisco?\"\n",
    "    results_version: Optional[int] = 1\n",
    "    context_lengths_min: Optional[int] = 800\n",
    "    context_lengths_max: Optional[int] = 8100\n",
    "    context_lengths_num_intervals: Optional[int] = 10\n",
    "    context_lengths: Optional[list[int]] = None\n",
    "    document_depth_percent_min: Optional[int] = 0\n",
    "    document_depth_percent_max: Optional[int] = 100\n",
    "    document_depth_percent_intervals: Optional[int] = 4\n",
    "    document_depth_percents: Optional[list[int]] = None\n",
    "    document_depth_percent_interval_type: Optional[str] = \"linear\"\n",
    "    num_concurrent_requests: Optional[int] = 1\n",
    "    save_results: Optional[bool] = True\n",
    "    results_dir: Optional[str] = 'results'\n",
    "    save_contexts: Optional[bool] = True\n",
    "    contexts_dir: Optional[str] = 'contexts'\n",
    "    final_context_length_buffer: Optional[int] = 200\n",
    "    seconds_to_sleep_between_completions: Optional[float] = None\n",
    "    print_ongoing_status: Optional[bool] = True\n",
    "    # LangSmith parameters\n",
    "    eval_set: Optional[str] = \"multi-needle-eval-pizza-3\"\n",
    "    # Multi-needle parameters\n",
    "    multi_needle: Optional[bool] = False\n",
    "    needles: list[str] = field(default_factory=lambda: [\n",
    "        \" Figs are one of the secret ingredients needed to build the perfect pizza. \", \n",
    "        \" Prosciutto is one of the secret ingredients needed to build the perfect pizza. \", \n",
    "        \" Goat cheese is one of the secret ingredients needed to build the perfect pizza. \"\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_to_test(args: dict) -> ModelProvider:\n",
    "    \"\"\"\n",
    "    Determines and returns the appropriate model provider based on the provided command dictionnary.\n",
    "    \n",
    "    Args:\n",
    "        args (dict): The command line arguments parsed into a CommandArgs dataclass instance.\n",
    "        \n",
    "    Returns:\n",
    "        ModelProvider: An instance of the specified model provider class.\n",
    "    \n",
    "    Raises:\n",
    "        ValueError: If the specified provider is not supported.\n",
    "    \"\"\"\n",
    "    match args.provider.lower():\n",
    "        case \"openai\":\n",
    "            return OpenAI(model_name=args.model_name)\n",
    "        case \"anthropic\":\n",
    "            return Anthropic(model_name=args.model_name)\n",
    "        case \"databricks\":\n",
    "            return Databricks(model_name=args.model_name)\n",
    "        case _:\n",
    "            raise ValueError(f\"Invalid provider: {args.provider}\")\n",
    "\n",
    "def get_evaluator(args: dict) -> Evaluator:\n",
    "    \"\"\"\n",
    "    Selects and returns the appropriate evaluator based on the provided command arguments.\n",
    "    \n",
    "    Args:\n",
    "        args (CommandArgs): The command line arguments parsed into a CommandArgs dataclass instance.\n",
    "        \n",
    "    Returns:\n",
    "        Evaluator: An instance of the specified evaluator class.\n",
    "        \n",
    "    Raises:\n",
    "        ValueError: If the specified evaluator is not supported.\n",
    "    \"\"\"\n",
    "    match args.evaluator_label.lower():\n",
    "        case \"openai\":\n",
    "            return OpenAIEvaluator(model_name=args.evaluator_model_name,\n",
    "                                   question_asked=args.retrieval_question,\n",
    "                                   true_answer=args.needle)\n",
    "        case \"langsmith\":\n",
    "            return LangSmithEvaluator()\n",
    "        case _:\n",
    "            raise ValueError(f\"Invalid evaluator: {args.evaluator}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing single-needle\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Needle, haystack, and retrieval_question must be provided.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m: \n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTesting single-needle\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 9\u001b[0m     tester \u001b[38;5;241m=\u001b[39m \u001b[43mLLMNeedleHaystackTester\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCommandDict\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/repos/LLMTest_NeedleInAHaystack/needlehaystack/llm_needle_haystack_tester.py:69\u001b[0m, in \u001b[0;36mLLMNeedleHaystackTester.__init__\u001b[0;34m(self, model_to_test, evaluator, needle, haystack_dir, retrieval_question, results_version, context_lengths_min, context_lengths_max, context_lengths_num_intervals, context_lengths, document_depth_percent_min, document_depth_percent_max, document_depth_percent_intervals, document_depth_percents, document_depth_percent_interval_type, num_concurrent_requests, save_results, save_contexts, final_context_length_buffer, seconds_to_sleep_between_completions, print_ongoing_status, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA language model must be provided to test.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m needle \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m haystack_dir \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m retrieval_question:\n\u001b[0;32m---> 69\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNeedle, haystack, and retrieval_question must be provided.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneedle \u001b[38;5;241m=\u001b[39m needle\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhaystack_dir \u001b[38;5;241m=\u001b[39m haystack_dir\n",
      "\u001b[0;31mValueError\u001b[0m: Needle, haystack, and retrieval_question must be provided."
     ]
    }
   ],
   "source": [
    "args = CommandArgs\n",
    "args.model_to_test = get_model_to_test(args)\n",
    "args.evaluator = get_evaluator(args)\n",
    "\n",
    "if args.multi_needle == True:\n",
    "    print(\"Testing multi-needle\")\n",
    "    tester = LLMMultiNeedleHaystackTester(**args.__dict__)\n",
    "else: \n",
    "    print(\"Testing single-needle\")\n",
    "    tester = LLMNeedleHaystackTester(**args.__dict__)\n",
    "tester.start_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tester.start_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
